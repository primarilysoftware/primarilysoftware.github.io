---
layout: post
title: "A Bug's Tale"
comments: true
description: "The story of one particularly interesting bug"
keywords: ".net, dotnet, wcf, bug"
---
Working in a production support role can take you on some wild rides.  At times, hunting down the source of a bug can make
you want to rip out your hair and even question your sanity.  Quite often though, the harder you work to
unravel a problem, the sweeter the satisfaction when you finally get to the bottom of it.  This is the story of one of those
kind of bugs.  While it may have cost me a few hair folicles, the payoff at the end was worth it. 

### How it all started
I work for a fairly small software development shop.  We support a handful of different products, including
a desktop app that uses WCF to call back home to our service layer.  Out of the blue, we got a report from a user
saying they were getting timeout errors in the desktop software related to some functions that clalled back to 
our WCF service.  Initially, our support team thought this must be a fluke.  If our service was down, we would have
been flooded with calls.  This was just a single desktop, in an office with several installations.  Being desktop
software, support frequently gets calls from users blaming our software for any and every PC related problem they 
may have.  We hadn't released any updates to this part of the app for months, so support felt pretty confident that
this was not really our problem.

Nonetheless, our company prides itself in customer service, so our support team will try to help as much as they can.
They started to look at the usual suspects when network related errors pop up: antivirus, SSL/TLS cert issues, bad
internet connection.  Nothing obvious turned up.  In the mean time, new reports of this same issue started to come in.
The numbers were small, maybe 20 of our 10,000 or so installs were having problems.  Not enough to make this a critical
issue, but enough that we couldn't write this off as some fluke.

Thats when support called for ["the expert"](https://www.youtube.com/watch?v=BKorP55Aqvg).
Unfortunately, our expert in this area was gone.  He had moved on to another company several months back.  Noone else knew much
about this particular area of the code in the desktop app, certainly not me.  I happened to work on the service this code was
calling back to, so naturally, I became the reluctant heir to this problem.

###Digging In
I have worked with WCF enough to know she can be a fickle mistress.  My first thought was some configuration issue.
That led nowhere.  I have also known WCF to bury the real error message, so switched on trace logging.  Still nothing
very helpful.  The only error in the WCF trace log was:

```csharp
The request channel timed out while waiting for a reply after 00:00:59.8595997. Increase the timeout value passed
to the call to Request or increase the SendTimeout value on the Binding. The time allotted to this operation may
have been a portion of a longer timeout.
```

This wasn't much to go on.  There was no reason for this request to be timing out.  The vast majority of our clients were
able to connect to the service just fine.  I trolled through our server side logs, and couldn't find any evidence that the request was
ever making it to our servers.  OK, so the the request must be getting dropped somewhere along the way.  It is not unheard of for an
antivirus to drop requests like this, so we explored that avenue some more.  We went as far as temporarily disabling antivirus,
but to no avail.

I needed more information.  We were able to get System.Net trace logs and a Wireshark trace.  The System.Net trace showed
a different error:

```csharp
System.Net.Sockets Error: 0 : [1916] Exception in Socket#45648486::Send - An existing connection was forcibly closed by the remote host.
```

So why was the connection getting reset?  The Wireshark trace showed something interesting:

![Wireshark trace]()

The TCP connection was being opened, then nothing.  10 seconds of nothing.  And then the connection was reset by the server.
Since the client is connecting over https, you would expect to see the SSL/TLS handshake start immediately after establishing
the TCP connection.  But here we see nothing.

At this point, I was stumped.  I could not explain what I was seeing, and I was starting to feel like I was in over my head.  I
decided to try and isolate the problem, running a series of smaller tests to get a better idea for where things were going
wrong.

Can the client browse to the service endpoint in IE?  That worked fine.  If nothing else, that confirmed that the client could
reach our servers, as well as establish a secure https connection, at least sometimes.

Maybe this was a SSL/TLS protocol issue?  I have seen that before.  After the POODLE vulnerablity was published a few years ago,
we ran into this kind of problem several times in areas of our apps that communicated with outside vendors.  Servers would be
updated, dropping support for less secure protocols, sometimes leaving us scrambling to get our end up to date as well.

I really thought I was on to something here.  Our app targeted .NET 4.0, which I knew only supported SSL 3.0 and TLS 1.0 by default.
Maybe our infrastructure guys did a surprise update to our servers, dropping TLS 1.0?  Or maybe these desktops had some fancy
new antivirus/firewall that intercepted insecure traffic?  I didn't spend much time asking questions, because I thought I had
this case figured out.  From my experience following POODLE, I knew that apps targeting .NET 4.0 did not officially support
TLS 1.1 or TLS 1.2, but as long as .NET 4.5 was installed on the machine, there is a
[little trick](https://stackoverflow.com/questions/28286086/default-securityprotocol-in-net-4-5) you can use to get these
protocols working:

```csharp
ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls | (SecurityProtocolType)768 | (SecurityProtocolType)3072;
```

The new enum values for TLS 1.1 and TLS 1.2 were added in .NET 4.5, and are not visible to apps targeting .NET 4.0.  However
since .NET 4.5 is an inplace update to the .NET framework, as long as .NET 4.5 is installed on the machine, this little hack
will work.  I put a release together with this change, and got it out to one of our broken clients to test, feeling confident
I was about to be a hero.

I was quickly disappointed.  This change made no difference.  I was stubborn though, and desperate.  I couldn't give up on 
this line of thinking, partly because I couldn't come up with any other plausible explanations for why these clients were
having issues.

## A Controlled Experiment
The next question I wanted to answer was, could these clients make a WCF call to our service using _any_ SSL/TLS protocol?
I decided to put together a test app to gather information in a more controlled way.  I wrote a little .NET 4.6
console app that attempted a series of WCF calls, switching from SSL 3.0, to TLS 1.0, to TLS 1.1, to TLS 1.2.

As I was putting together this test app, I started to develop another theory.  Our WCF service was configured to use `msbin` for the 
message encoding.  Maybe these clients had a firewall that was doing deep packet inspection (DPI), and was dropping these
packets because of the binary data?  I decided to test that theory out as well.  I exposed a plain XML endpoint for our
service, and added a second series of calls to the test app that went to this new XML endpoint, still cycling through all the 
different SSL/TLS protocols.

We got a chance to try this test app out on one of the affected machines, and lo and behold!  None of the calls worked...
I can't say that I wasn't disappointed with the result, but it did prove a few things in my mind.  This was not a problem
with something specific to our desktop app.  I had just built an entirely new app, fresh source code, and it too was
experiencing the same issues.  Also, I was now convinced that this had nothing to do with the SSL/TLS
protocol being used, and it seemed very unlikely that this was a antivirus/firewall issue.

### A More Improbable Explanation
I was ruling out some possible explanations, but I didn't feel like I was getting closer to finding what was actually
going wrong.  Reviewing all the information I had gathered up to this point, my test in the browser proved that these
machines could access our service, while the Wireshark trace showed that the SSL/TLS handshake was not starting, though my
test app suggested that this was not an issue related to any specific SSL/TLS protocol.

Something must have changed on these systems.  They were working fine one day, and stopped working the next.  We knew it
wasn't our code that changed.  We decided to start looking for other things that may have changed.  Two areas sprang to
mind in particular.  First, was there any other software that had been installed recently on these machines?  I could
imagine a scenario where some 3rd party app came in, mucked with the registry, and started breaking things.  The other
thing that came to mind was a problematic Windows patch.  Even Microsoft makes mistakes when building software (can you
believe that?), so it is not unheard of for a patch to cause unexpected issues.

There is a command you can use to get a list of hotfixes applied to a machine:

```
wmic qfe list full
```

I asked our support team to start gathering this data anytime they got a report of the issue.  This allowed me to look for
a two things.  Could we identify a patch common to all of these affected machines?  Which patches were installed around the 
the time the machine started having issues?  On the first machine we got this information from, there was one patch
that jumped off the page,
[KB4014504](https://support.microsoft.com/en-us/help/4014504/description-of-the-security-and-quality-rollup-for-the-net-framework-3).
While this patch said it was for .NET 3.5, everything else about it seemed suspect.  Microsoft's description of the patch was:

> This security update for the Microsoft .NET Framework resolves a security feature bypass vulnerability in which the .NET Framework
> (and the .NET Core) components do not completely validate certificates.

So the patch was directly related to SSL/TLS, and it had been installed on this machine the same day they started seeing
issues.  Digging into the details of what changed a little deeper, with this change .NET would start validating  the
[EKU extension](https://tools.ietf.org/html/rfc5280#section-4.2.1.12) of a certificate.  I had never heard of this extension
before, so I quickly read through the proposal and tried to verify that our cert had been issued properly.  I also passed
this along to our infrustructure team, and they assurred me that everything was in order with our cert.  I was still skeptical,
the circumstances surrounding this patch seemed 

but I could not for the life
of me get a WCF call to succeed.  This made me think that maybe WCF was the problem.  I decided to try and remove WCF
from the call stack altogether.  I hacked together some code to call our service using the `TcpClient` and `SslStream`
classes.

```csharp
using (var tcpClient = new TcpClient("our.domain.name", 443))
using (var sslStream = new SslStream(tcpClient.GetStream()))
{
    sslStream.AuthenticateAsClient("our.domain.name");
    var request = ...;
    sslStream.Write(request);
}
```

We tried this out on one of the affected machines, and again it didn't work.  So whatever the issue was, it was beneath
the WCF stack.  The call to `AuthenticateAsClient` was taking 30 seconds.  This lined up perfectly with what we saw in 
the Wireshark trace, but I couldn't figure out why this was happening.

I was starting to think this must be a bug deep down in the .NET framework.  Since the issue was occurring in the
`AuthenticateAsClient` call, I started to look for alternative SSL/TLS implementations.  This led me to the
(Bouncy Castle library)[http://www.bouncycastle.org/csharp/].

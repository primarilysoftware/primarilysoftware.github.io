---
layout: post
title: "A Bug's Tale"
comments: true
description: "The story of one particularly interesting bug"
keywords: ".net, dotnet, wcf, bug"
---
Working in a production support role can take you on some wild rides.  At times, hunting down the source of a bug can make
you want to rip out your hair and even question your sanity.  Quite often though, the harder you work to
unravel a problem, the sweeter the satisfaction when you finally get to the bottom of it.  This is the story of one of those
kind of bugs.  While it may have cost me a few hair folicles, the payoff at the end was worth it. 

### How it all started
I work for a fairly small software development shop.  We support a handful of different products, including
a desktop app that uses WCF to call back home to our service layer.  Out of the blue, support started to see
reports trickling in where users were getting errors in the desktop software calling our WCF service.  The first
time support saw one of these reports, they thought it was a fluke.  Being desktop software, support frequently get calls
from users blaming our software for any and every PC related problem.  We hadn't released any updates to this part of
the app for months, so support felt pretty confident that this was not really our problem.

Nonetheless, our company prides itself in customer service, so our support team will try to help as much as they can.
They started to look at the usual suspects when network related errors pop up: antivirus, SSL/TLS cert issues, bad
internet connection.  Nothing obvious turned up.  In the mean time, new reports of this same issue started to come in.
The numbers were small, maybe 20 of our 10,000 or so installs were having problems.  Not enough to make this a critical
issue, but enough that we couldn't write this off as some fluke.

Thats when support called for ["the expert"](https://www.youtube.com/watch?v=BKorP55Aqvg).
Unfortunately, our expert in this area was gone.  He had moved on to another company several months back.  Noone else knew much
about this particular area of the code in the desktop app, certainly not me.  I happened to work on the service this code was
calling back to, so naturally, I was the unfortunate heir to this inheritance.

I have worked with WCF enough to know she can be a fickle mistress.  My first thought was some configuration issue.
That led nowhere.  I have also known WCF to bury the real error message, so switched on trace logging.  Still nothing
very helpful.  The only error in the WCF trace log was:

```csharp
The request channel timed out while waiting for a reply after 00:00:59.8595997. Increase the timeout value passed
to the call to Request or increase the SendTimeout value on the Binding. The time allotted to this operation may
have been a portion of a longer timeout.
```

This wasn't much to go on.  I trolled through the server side logs, and couldn't find any evidence that the request was
ever making it to our servers.  OK, so the the request must be getting dropped somewhere along the way.  It is not unheard of for an
antivirus to drop requests like this, so we explored that avenue some more.  We went as far as temporarily disabling antivirus,
but to no avail.

I needed more information.  We were able to get System.Net trace logs and a Wireshark trace.  The System.Net trace showed
a different error:

```csharp
System.Net.Sockets Error: 0 : [1916] Exception in Socket#45648486::Send - An existing connection was forcibly closed by the remote host.
```

So why was the connection getting reset?  The Wireshark trace showed something interesting:

![Wireshark trace]()

The TCP connection was being opened, then nothing.  10 seconds of nothing.  And then the connection was reset by the server.
Since the client is connecting over https, you would expect to see the SSL/TLS handshake start immediately after establishing
the connection.  But here we see nothing.

At this point, I was stumped.  I could not explain what I was seeing, and I was starting to feel like I was in over my head.  I
decided to try and isolate the problem, running a series of smaller tests to get a better idea for where things were going
wrong.

Can the client browse to the service endpoint in IE?  That worked fine.  If nothing else, that confirmed that the client could
reach our servers, as well as establish a secure https connection, at least sometimes.

Maybe this was a SSL/TLS protocol issue?  I have seen that before.  After the POODLE vulnerablity was published a few years ago,
we ran into this kind of problem several times in areas of our apps that communicated with outside vendors.  Servers would be
updated, dropping support for less secure protocols, sometimes leaving us scrambling to get our end up to date as well.

I really thought I was on to something here.  Our app targeted .NET 4.0, which I knew only supported SSL 3.0 and TLS 1.0 by default.
Maybe our infrastructure guys did a surprise update to our servers, dropping TLS 1.0?  Or maybe these desktops had some fancy
new antivirus/firewall that intercepted insecure traffic?  I didn't spend much time asking questions, because I thought I had
this case figured out.  From my experience following POODLE, I knew that apps targeting .NET 4.0 did not officially support
TLS 1.1 or TLS 1.2, but as long as .NET 4.5 was installed on the machine, there is a
[little trick](https://stackoverflow.com/questions/28286086/default-securityprotocol-in-net-4-5) you can use to get these
protocols working:

```csharp
ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls | (SecurityProtocolType)768 | (SecurityProtocolType)3072;
```

The new enum values for TLS 1.1 and TLS 1.2 were added in .NET 4.5, and are not visible to apps targeting .NET 4.0.  However
since .NET 4.5 is an inplace update to the .NET framework, as long as .NET 4.5 is installed on the machine, this little hack
will work.  I put a release together with this change, and got it out to one of our broken clients to test, feeling confident
I would was about to be named a hero.

I was quickly disappointed.  This change made no difference.  I am stubborn though, and wasn't ready to give up on this line
of thinking.  
